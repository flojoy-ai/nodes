{
  "description": "From here, in conjunction with a loop, we iterate over all the files found, and return each one by file path as a TextBlob.\n\nThe TextBlob can be recognized as an optional input to the LOCAL_FILE node, which can then load the file at that path and return the appropriate datatype.",
  "parameters": [
    {
      "name": "current_iteration",
      "type": "Scalar",
      "description": "This is the input from the LOOP_INDEX node that determines\nwhether we need to initialize this routine or not."
    },
    {
      "name": "default_params",
      "type": "DefaultParams",
      "description": "This provides the node_id so that we can identify which\nobject in SmallMemory to pull (for example, unique identification\nof this node if there are multiple BATCH_PROCESSOR nodes)."
    },
    {
      "name": "directory_path",
      "type": "str",
      "description": "The directory in which we should match the pattern to find the files."
    },
    {
      "name": "pattern",
      "type": "str",
      "description": "The glob pattern to match.\nIf not provided, all files in the directory are returned.\nThe current implementation supports recursion and double wildcard matching."
    },
    {
      "name": "refresh",
      "type": "bool",
      "description": "A switching parameter that refreshes the cache of files. If a separate\nprogramme is expected to continuously write new files of interest to the\ndirectory, this flag will enable the update of the new files for processing."
    }
  ],
  "returns": [
    {
      "name": "fname",
      "type": "TextBlob",
      "description": "The file name on the current iteration."
    },
    {
      "name": "n_files",
      "type": "Scalar",
      "description": "The total number of files matched by the pattern in the given directory."
    }
  ],
  "code": "from flojoy import flojoy, Scalar, SmallMemory, DefaultParams, TextBlob\nimport glob\nfrom typing import Any, TypedDict\n\nmemory_key = \"batch-processor-info\"\n\n\nclass BATCH_OUTPUT(TypedDict):\n    fname: TextBlob\n    n_files: Scalar\n\n\ndef get_fnames(d, p):\n    return [file for file in glob.glob(d + \"/\" + p, recursive=True)]\n\n\n@flojoy(inject_node_metadata=True)\ndef BATCH_PROCESSOR(\n    current_iteration: Scalar,\n    default_params: DefaultParams,\n    directory_path: str,\n    pattern: str = \"\",\n    refresh: bool = True,\n) -> BATCH_OUTPUT:\n    \"\"\"The BATCH_PROCESSOR node is designed to glob match a pattern in the given input directory.\n\n    From here, in conjunction with a loop, we iterate over all the files found, and return each one by file path as a TextBlob.\n\n    The TextBlob can be recognized as an optional input to the LOCAL_FILE node, which can then load the file at that path and return the appropriate datatype.\n\n    Parameters\n    ----------\n    current_iteration : Scalar\n        This is the input from the LOOP_INDEX node that determines\n        whether we need to initialize this routine or not.\n    default_params : DefaultParams\n        This provides the node_id so that we can identify which\n        object in SmallMemory to pull (for example, unique identification\n        of this node if there are multiple BATCH_PROCESSOR nodes).\n    directory_path : str\n        The directory in which we should match the pattern to find the files.\n    pattern : str\n        The glob pattern to match.\n        If not provided, all files in the directory are returned.\n        The current implementation supports recursion and double wildcard matching.\n    refresh : bool\n        A switching parameter that refreshes the cache of files. If a separate\n        programme is expected to continuously write new files of interest to the\n        directory, this flag will enable the update of the new files for processing.\n\n    Returns\n    -------\n    fname : TextBlob\n        The file name on the current iteration.\n    n_files : Scalar\n        The total number of files matched by the pattern in the given directory.\n    \"\"\"\n\n    node_id = default_params.node_id\n    curr_iter = current_iteration.c\n    # if iteration 1, pattern find, then write to SmallMemory\n    if curr_iter == 1:\n        files = get_fnames(directory_path, pattern if pattern else \"*\")\n        return BATCH_OUTPUT(\n            fname=TextBlob(text_blob=\"\"), n_files=Scalar(c=len(files) + 1)\n        )\n    elif curr_iter == 2:  # loop index starts at 1, sigh\n        files = get_fnames(directory_path, pattern if pattern else \"*\")\n        SmallMemory().write_to_memory(\n            node_id,\n            memory_key,\n            {\n                \"node_id\": node_id,\n                \"current_iteration\": curr_iter,\n                \"files\": files,\n                \"original_files\": files,\n                \"n_files\": int(len(files)),\n            },\n        )\n    # if refresh, glob again, read from smallmemory,\n    # find difference, append difference to files in SmallMemory\n    if refresh:\n        new_files = get_fnames(directory_path, pattern if pattern else \"*\")\n        old_data: dict[str, Any] = SmallMemory().read_memory(node_id, memory_key) or {}\n        if old_data:\n            difference = set(new_files).difference(\n                set(old_data[\"original_files\"])\n            )  # designed to only catch the addition of files\n            if not all([not d in old_data[\"original_files\"] for d in list(difference)]):\n                # this means there are more new files added to the mix\n                SmallMemory().write_to_memory(\n                    node_id,\n                    memory_key,\n                    {\n                        \"node_id\": node_id,\n                        \"current_iteration\": curr_iter,\n                        \"files\": old_data[\"files\"] + list(difference),\n                        \"original_files\": old_data[\"original_files\"],\n                        \"n_files\": int(len(old_data[\"original_files\"])),\n                    },\n                )\n\n    # Now we read from SmallMemory and pop fname\n    data: dict[str, Any] = SmallMemory().read_memory(node_id, memory_key) or {}\n    fname = data[\"files\"].pop(0)\n    # Now write to SmallMemory for the next iteration\n    data[\"current_iteration\"] = curr_iter\n    SmallMemory().write_to_memory(node_id, memory_key, data)\n    if curr_iter > data[\"n_files\"]:\n        SmallMemory().delete_object(node_id, memory_key)\n    # And return the current fname\n    return BATCH_OUTPUT(\n        fname=TextBlob(text_blob=fname), n_files=Scalar(c=len(data[\"original_files\"]))\n    )\n"
}