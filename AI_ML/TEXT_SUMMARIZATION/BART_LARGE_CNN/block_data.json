{
  "description": "The \"summary_text\" column contains a summary of the text in the corresponding row of the input dataframe.",
  "parameters": [],
  "returns": [
    {
      "name": null,
      "type": "DataFrame",
      "description": "dataframe containing the summary text in the \"summary_text\" column"
    }
  ],
  "code": "from flojoy import flojoy, run_in_venv, DataFrame\n\n\n@flojoy\n@run_in_venv(\n    pip_dependencies=[\n        \"transformers==4.30.2\",\n        \"torch==2.0.1\",\n        \"torchvision==0.15.2\",\n        \"pandas\",\n    ]\n)\ndef BART_LARGE_CNN(default: DataFrame) -> DataFrame:\n    \"\"\"The BART_LARGE_CNN node takes an input dataframe with multiple rows and a single column, and produces a dataframe with a single \"summary_text\" column.\n\n    The \"summary_text\" column contains a summary of the text in the corresponding row of the input dataframe.\n\n    Returns\n    -------\n    DataFrame\n        dataframe containing the summary text in the \"summary_text\" column\n    \"\"\"\n\n    import torch\n    from flojoy import snapshot_download\n    from transformers import BartTokenizer, BartForConditionalGeneration\n    import pandas as pd\n\n    input_df = default.m\n\n    assert (\n        len(input_df.columns.tolist()) == 1\n    ), \"Can only take a single-column dataframe as input\"\n\n    # Load the repo from either the local cache or from the web, and get the local path\n    local_path = snapshot_download(\n        repo_id=\"facebook/bart-large-cnn\", revision=\"3d22493\"\n    )\n\n    # Load the pre-trained BART model\n    model = BartForConditionalGeneration.from_pretrained(local_path)\n    tokenizer = BartTokenizer.from_pretrained(local_path)\n\n    def _chunk_text(text):\n        inputs_no_trunc = tokenizer(\n            text, max_length=None, return_tensors=\"pt\", truncation=False\n        )\n        chunks = []\n        step = 1024\n        # step = tokenizer.model_max_length - 1\n        for i in range(0, len(inputs_no_trunc[\"input_ids\"][0]), step):\n            chunk = inputs_no_trunc[\"input_ids\"][0][i : i + step]\n            chunks.append(torch.unsqueeze(chunk, 0))\n        return chunks\n\n    def _summarize_text(text):\n        chunks = _chunk_text(text)\n        summary_ids = [\n            model.generate(\n                chunk,\n                num_beams=4,\n                max_length=1024 // 2,\n                early_stopping=True,\n            )\n            for chunk in chunks\n        ]\n        summaries = [\n            \"\\n\".join(\n                [\n                    tokenizer.decode(\n                        g, skip_special_tokens=True, clean_up_tokenization_spaces=False\n                    )\n                    for g in id\n                ]\n            )\n            for id in summary_ids\n        ]\n        return \"\\n\".join(summaries)\n\n    column = input_df.columns[0]\n\n    with torch.inference_mode():\n        output_df = pd.DataFrame(\n            input_df[column].apply(_summarize_text).rename(\"summary_text\")\n        )\n    return DataFrame(df=output_df)\n"
}