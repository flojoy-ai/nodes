{
  "description": "The input image is expected to be a DataContainer of an 'image' type.\n\nThe output is a DataContainer of an 'image' type with the same dimensions as the input image, but with the red, green, and blue channels replaced with the segmentation mask.",
  "parameters": [],
  "returns": [
    {
      "name": null,
      "type": "Image",
      "description": null
    }
  ],
  "code": "from flojoy import flojoy, run_in_venv, Image\n\n\n@flojoy\n@run_in_venv(\n    pip_dependencies=[\n        \"torch==2.0.1\",\n        \"torchvision==0.15.2\",\n        \"Pillow\",\n        \"numpy\",\n    ]\n)\ndef DEEPLAB_V3(default: Image) -> Image:\n    \"\"\"The DEEPLAB_V3 node returns a segmentation mask from an input image in a dataframe.\n\n    The input image is expected to be a DataContainer of an 'image' type.\n\n    The output is a DataContainer of an 'image' type with the same dimensions as the input image, but with the red, green, and blue channels replaced with the segmentation mask.\n\n    Returns\n    -------\n    Image\n    \"\"\"\n\n    import os\n    import numpy as np\n    import PIL.Image\n    import torch\n    from torchvision import transforms\n    import torchvision.transforms.functional as TF\n    from flojoy import Image\n    from flojoy.utils import FLOJOY_CACHE_DIR\n\n    # Parse input image\n    input_image = default\n    r, g, b, a = input_image.r, input_image.g, input_image.b, input_image.a\n    nparray = (\n        np.stack((r, g, b, a), axis=2) if a is not None else np.stack((r, g, b), axis=2)\n    )\n    # Convert input image\n    input_image = TF.to_pil_image(nparray).convert(\"RGB\")\n    # Set torch hub cache directory\n    torch.hub.set_dir(os.path.join(FLOJOY_CACHE_DIR, \"cache\", \"torch_hub\"))\n    model = torch.hub.load(\n        \"pytorch/vision:v0.15.2\",\n        \"deeplabv3_resnet50\",\n        pretrained=True,\n        skip_validation=True,\n    )\n    model.eval()\n    # Preprocessing\n    preprocess_transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n    # Feed the input image to the model\n    input_tensor = preprocess_transform(input_image)\n    input_batch = input_tensor.unsqueeze(0)\n    with torch.inference_mode():\n        output = model(input_batch)[\"out\"][0]\n    # Fetch the output\n    output_predictions = output.argmax(0)\n    palette = torch.tensor([2**25 - 1, 2**15 - 1, 2**21 - 1])\n    colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n    colors = (colors % 255).numpy().astype(\"uint8\")\n    # plot the semantic segmentation predictions of 21 classes in each color\n    r = PIL.Image.fromarray(output_predictions.byte().cpu().numpy()).resize(\n        input_image.size\n    )\n    r.putpalette(colors)\n    out_img = np.array(r.convert(\"RGB\"))\n    # Build the output image\n    return Image(\n        r=out_img[:, :, 0],\n        g=out_img[:, :, 1],\n        b=out_img[:, :, 2],\n        a=None,\n    )\n"
}