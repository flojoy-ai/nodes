{
  "description": "The ability to find local peaks will not depend on the extrema being\n    exponentially separated from the neighboring values, or some other restrictive constraint.\n\n    We implement three algorithms to find the local max. The first algorithm uses a masked phase\n    cross correlation technique [1], while the second uses the persistence birth/death algorithms [2, 3].\n    The original implementations of these libraries were utilized for the detection of elastic\n    scattering peaks in diffraction data, found in the 'scikit-ued' library of Python [4].\n\n    Note that the algorithm assumes that the extrema are symmetrically distributed around\n    a center point. All extrema are determined relative to the center position.\n    Also, for closely spaced points, noisy data, or data that has a very high dynamic range, the\n    algorithm fails. Therefore, this approach is suited only for images with\n    high degrees of symmetry and reasonable contrast.\n\n    Therefore, we also use the second prominence algorithm, where a single\n    value is applied locally to determine the relative 'peakiness' of a given pixel,\n    inspecting only the neighbors around that given pixel. While computationally\n    more intense for images with a resolution of >4K, it produces extremely accurate\n    results for the correct value of prominence in potentially low-contrast images.\n    By definition, it is a local pixel algorithm, and therefore does not perform any blob detection,\n    unlike the high-symmetry algorithm which creates high contrast in\n    the image with laplacian filtering, and identifies regions of this high contrast image.\n\n    Note, however, that the persistence algorithm tends to find more points than what are actually there.\n    For images with high frequency components (i.e. quickly varying values among the third nearest neighbours),\n    the algorithm will tend to identify each as a 'peak', even though the local maxima is elsewhere. This\n    therefore assumes that the image has been properly preprocessed with another image processing node to\n    provide a sufficient low-frequency image such that the prominence of each pixel is well defined.\n\n    To combat this limitation, we present the most robust of the algorithms that should work on images\n    of low or high contrast, low or high frequency components, and of low or high dynamic range. It is\n    computationally more expensive, as it involves repeated convolutions of the image, but it is\n    the most reliable of the methods for a general image.\n\n    This routine is known as the Laplacian of Gaussian algorithm [5].\n    The key to this algorithm is to apply a filter specially chosen such that regions around peaks have high\n    levels of contrast (essentially binarize the image around its peak so that near the peak, the\n    image is one, and zero otherwise). To achieve such a filter, the Laplacian of a Gaussian is used:\n\n    .. math:: \nabla^2 L \\equiv L_{xx} + L_{yy}\n\n    which yields the following filter (for a Gaussian of width sigma, centered at the origin):\n\n    ..math :: LG = -\frac{1}{\\pi\\sigma^4}\big[1-\frac{x^2+y^2}{2\\sigma^2}\big]e^{-\frac{x^2+y^2}{2\\sigma^2}}\n\n    The output of this filter will be a maximum where there is an edge from a peak, the maximum response\n    of which is given for 1.41*'blob radius' around the peak.\n    Applying this filter repeatedly with varying degrees of sigma, will continue to refine the edges around\n    the peak until the image is essentially binarized around the peaks. Due to the repeated convolutions,\n    this algorithm is generally expensive, but specific methods have been implemented using FFT to speed\n    up these calculations.\n\n\n    Parameters\n    ----------\n\n    default : Image | Grayscale | Matrix\n        The input DataContainer that contains the image to be processed.\n        Can either be RGBA, greyscale, or a matrix type.\n        In the case of RGB(A), the image is flattened to grayscale for the peak detection.\n    image_mask : Grayscale | Matrix\n        This object provides a mask to apply to the peak finding routines.\n        Peaks found by any algorithm inside this mask are ignored.\n        Should be of a datatype that can be static cast to booleans.\n        If none, it is assumed that the entire image is valid for peak detection.\n    center : list[int]\n        For the high symmetry algorithm, this provides the center of symmetry\n        to pass to the cross correlation routines.\n        If none is provided, an autocenter routine is run to attempt to find the center of symmetry.\n    min_dist : float\n        The minimum distance between peaks.\n        If the L2 distance (in pixels) of any pair of peaks is less than min_dist,\n        they are considered to be the same, and one is discarded.\n        This parameter applies to all algorithms.\n    algorithm : str\n        The name of the algorithm to use.\n    prominence : float\n        In the prominence and Laplacian of Gaussian algorithms, this defines the threshold\n        above or below which objects must pass in order to be considered a peak.\n        Does not apply to the high_symmetry algorithm.\n    k : float\n        This specifies the scaling of Gaussian filters between successive applications\n        of Gaussian filters of increasing sigma.\n        Default is chosen for ideal spherically symmetric peaks.\n        Can be tuned for more bizarre looking peak structures.\n        Applies only to the Laplacian of Gaussian algorithm.\n    sigma : float\n        The baseline standard deviation of the Gaussian filters,\n        only for the Laplacian of Gaussian algorithm.\n    max_power : int\n        Describes the upper limit of the degree of exponentiation for the successive\n        application of filters, only in the Laplacian of Gaussian algorithm.\n\n    Returns\n    -------\n    fig : Plotly\n        The Plotly figure so that the image can be visualized with its found peaks.\n    blobs : Grayscale\n        A blob mask that returns the regions around the found peaks.\n        It is only valid for the high_symmetry and log routines.\n        As the persistence algorithm is by definition hyperlocal, it has no notion of blobs\n        throughout the detection process, and as such returns a unity mask.\n\n    References\n    ----------\n    [1] Liu, Lai Chung. Chemistry in Action: Making Molecular Movies with Ultrafast\n    Electron Diffraction and Data Science, Chapter 2. Springer Nature, 2020.\n\n    [2] Huber, S. (2021). Persistent Homology in Data Science. In: Haber, P.,\n    Lampoltshammer, T., Mayr, M., Plankensteiner, K. (eds) Data Science - Analytics\n    and Applications. Springer Vieweg, Wiesbaden.\n    https://doi.org/10.1007/978-3-658-32182-6_13\n\n    [3] Edelsbrunner, H. and John L Harer (2010). Computational Topology. In: American\n    Mathematical Society.\n\n    [4] L. P. Ren\u00e9 de Cotret, M. R. Otto, M. J. Stern. and B. J. Siwick.\n    An open-source software ecosystem for the interactive exploration of\n    ultrafast electron scattering data, Advanced Structural and Chemical\n    Imaging 4:11 (2018)\n\n    [5] https://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian",
  "parameters": [],
  "returns": [],
  "code": "from itertools import combinations\nfrom math import floor\nfrom os import cpu_count\nfrom typing import Literal, Optional, TypedDict\nfrom warnings import catch_warnings, simplefilter\n\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport scipy.fft\nimport skimage.filters as filters\nfrom flojoy import DCNpArrayType, Grayscale, Image, Matrix, Plotly, flojoy\nfrom nodes.VISUALIZERS.template import plot_layout\nfrom PIL import Image as PILImage\nfrom scipy import spatial\nfrom scipy.fft import _pocketfft\nfrom scipy.ndimage import gaussian_filter, laplace, shift\nfrom scipy.signal import fftconvolve\nfrom skimage.draw import ellipse\nfrom skimage.measure import label, regionprops\nfrom skimage.morphology import binary_erosion, disk\nfrom skimage.registration import phase_cross_correlation\nfrom skimage.registration._masked_phase_cross_correlation import cross_correlate_masked\n\n\nclass EXTREMA_OUTPUT(TypedDict):\n    fig: Plotly\n    blobs: Grayscale\n\n\n@flojoy(deps={\"scikit-image\": \"0.21.0\"}, node_type=\"VISUALIZERS\")\ndef EXTREMA_DETERMINATION(\n    default: Image | Grayscale | Matrix,\n    image_mask: Optional[Grayscale | Matrix] = None,\n    center: Optional[list[int]] = None,\n    min_dist: float = 0.0,\n    algorithm: Literal[\"persistence\", \"high_symmetry\", \"log\"] = \"log\",\n    prominence: float = 0.0,\n    k: float = 1.41,\n    sigma: float = 1.0,\n    max_power: int = 9,\n) -> EXTREMA_OUTPUT:\n    \"\"\"The EXTREMA_DETERMINATION node is used to determine the peak in an image.\n\n    The ability to find local peaks will not depend on the extrema being\n    exponentially separated from the neighboring values, or some other restrictive constraint.\n\n    We implement three algorithms to find the local max. The first algorithm uses a masked phase\n    cross correlation technique [1], while the second uses the persistence birth/death algorithms [2, 3].\n    The original implementations of these libraries were utilized for the detection of elastic\n    scattering peaks in diffraction data, found in the 'scikit-ued' library of Python [4].\n\n    Note that the algorithm assumes that the extrema are symmetrically distributed around\n    a center point. All extrema are determined relative to the center position.\n    Also, for closely spaced points, noisy data, or data that has a very high dynamic range, the\n    algorithm fails. Therefore, this approach is suited only for images with\n    high degrees of symmetry and reasonable contrast.\n\n    Therefore, we also use the second prominence algorithm, where a single\n    value is applied locally to determine the relative 'peakiness' of a given pixel,\n    inspecting only the neighbors around that given pixel. While computationally\n    more intense for images with a resolution of >4K, it produces extremely accurate\n    results for the correct value of prominence in potentially low-contrast images.\n    By definition, it is a local pixel algorithm, and therefore does not perform any blob detection,\n    unlike the high-symmetry algorithm which creates high contrast in\n    the image with laplacian filtering, and identifies regions of this high contrast image.\n\n    Note, however, that the persistence algorithm tends to find more points than what are actually there.\n    For images with high frequency components (i.e. quickly varying values among the third nearest neighbours),\n    the algorithm will tend to identify each as a 'peak', even though the local maxima is elsewhere. This\n    therefore assumes that the image has been properly preprocessed with another image processing node to\n    provide a sufficient low-frequency image such that the prominence of each pixel is well defined.\n\n    To combat this limitation, we present the most robust of the algorithms that should work on images\n    of low or high contrast, low or high frequency components, and of low or high dynamic range. It is\n    computationally more expensive, as it involves repeated convolutions of the image, but it is\n    the most reliable of the methods for a general image.\n\n    This routine is known as the Laplacian of Gaussian algorithm [5].\n    The key to this algorithm is to apply a filter specially chosen such that regions around peaks have high\n    levels of contrast (essentially binarize the image around its peak so that near the peak, the\n    image is one, and zero otherwise). To achieve such a filter, the Laplacian of a Gaussian is used:\n\n    .. math:: \\nabla^2 L \\equiv L_{xx} + L_{yy}\n\n    which yields the following filter (for a Gaussian of width sigma, centered at the origin):\n\n    ..math :: LG = -\\frac{1}{\\pi\\sigma^4}\\big[1-\\frac{x^2+y^2}{2\\sigma^2}\\big]e^{-\\frac{x^2+y^2}{2\\sigma^2}}\n\n    The output of this filter will be a maximum where there is an edge from a peak, the maximum response\n    of which is given for 1.41*'blob radius' around the peak.\n    Applying this filter repeatedly with varying degrees of sigma, will continue to refine the edges around\n    the peak until the image is essentially binarized around the peaks. Due to the repeated convolutions,\n    this algorithm is generally expensive, but specific methods have been implemented using FFT to speed\n    up these calculations.\n\n\n    Parameters\n    ----------\n\n    default : Image | Grayscale | Matrix\n        The input DataContainer that contains the image to be processed.\n        Can either be RGBA, greyscale, or a matrix type.\n        In the case of RGB(A), the image is flattened to grayscale for the peak detection.\n    image_mask : Grayscale | Matrix\n        This object provides a mask to apply to the peak finding routines.\n        Peaks found by any algorithm inside this mask are ignored.\n        Should be of a datatype that can be static cast to booleans.\n        If none, it is assumed that the entire image is valid for peak detection.\n    center : list[int]\n        For the high symmetry algorithm, this provides the center of symmetry\n        to pass to the cross correlation routines.\n        If none is provided, an autocenter routine is run to attempt to find the center of symmetry.\n    min_dist : float\n        The minimum distance between peaks.\n        If the L2 distance (in pixels) of any pair of peaks is less than min_dist,\n        they are considered to be the same, and one is discarded.\n        This parameter applies to all algorithms.\n    algorithm : str\n        The name of the algorithm to use.\n    prominence : float\n        In the prominence and Laplacian of Gaussian algorithms, this defines the threshold\n        above or below which objects must pass in order to be considered a peak.\n        Does not apply to the high_symmetry algorithm.\n    k : float\n        This specifies the scaling of Gaussian filters between successive applications\n        of Gaussian filters of increasing sigma.\n        Default is chosen for ideal spherically symmetric peaks.\n        Can be tuned for more bizarre looking peak structures.\n        Applies only to the Laplacian of Gaussian algorithm.\n    sigma : float\n        The baseline standard deviation of the Gaussian filters,\n        only for the Laplacian of Gaussian algorithm.\n    max_power : int\n        Describes the upper limit of the degree of exponentiation for the successive\n        application of filters, only in the Laplacian of Gaussian algorithm.\n\n    Returns\n    -------\n    fig : Plotly\n        The Plotly figure so that the image can be visualized with its found peaks.\n    blobs : Grayscale\n        A blob mask that returns the regions around the found peaks.\n        It is only valid for the high_symmetry and log routines.\n        As the persistence algorithm is by definition hyperlocal, it has no notion of blobs\n        throughout the detection process, and as such returns a unity mask.\n\n    References\n    ----------\n    [1] Liu, Lai Chung. Chemistry in Action: Making Molecular Movies with Ultrafast\n    Electron Diffraction and Data Science, Chapter 2. Springer Nature, 2020.\n\n    [2] Huber, S. (2021). Persistent Homology in Data Science. In: Haber, P.,\n    Lampoltshammer, T., Mayr, M., Plankensteiner, K. (eds) Data Science - Analytics\n    and Applications. Springer Vieweg, Wiesbaden.\n    https://doi.org/10.1007/978-3-658-32182-6_13\n\n    [3] Edelsbrunner, H. and John L Harer (2010). Computational Topology. In: American\n    Mathematical Society.\n\n    [4] L. P. Ren\u00e9 de Cotret, M. R. Otto, M. J. Stern. and B. J. Siwick.\n    An open-source software ecosystem for the interactive exploration of\n    ultrafast electron scattering data, Advanced Structural and Chemical\n    Imaging 4:11 (2018)\n\n    [5] https://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian\n    \"\"\"\n\n    if isinstance(default, Image):\n        r = default.r\n        g = default.g\n        b = default.b\n        a = default.a\n\n        if a is None:\n            image = np.stack((r, g, b), axis=2)\n        else:\n            image = np.stack((r, g, b, a), axis=2)\n        image = PILImage.fromarray(image)\n        image = np.array(\n            image.convert(\"L\"), dtype=np.uint8\n        )  # a greyscale image that can be processed\n    elif isinstance(default, Grayscale) or isinstance(default, Matrix):\n        image = np.array(default.m)  # explicit typing just to be extra safe\n\n    # This now produces an (M, N) array that we can then process! The algorthims\n    # have no specificity on input data type unlike the `REGION_PROPERTIES` node.\n    if image_mask is None:\n        mask = np.ones(image.shape)\n    else:\n        mask = image_mask.m\n        if mask.shape != image.shape:\n            raise IndexError(\"Provided mask is not the same shape as the input image.\")\n    blob_mask = np.zeros(image.shape, dtype=np.uint8)\n    match algorithm:\n        case \"high_symmetry\":\n            if center is None:\n                center = autocenter(im=image, mask=mask)\n                # Then we need to autodetermine the center of the iamge\n                # raise ValueError(\"For the crossed correlated mask algorithm, the center of the peaks \\\n                #                   in the image must be specified\")\n            im = np.array(image, copy=True, dtype=float)\n            im -= im.min()\n\n            with catch_warnings():\n                simplefilter(\"ignore\", category=RuntimeWarning)\n                im /= gaussian_filter(input=im, sigma=min(image.shape) / 20, truncate=2)\n            im = np.nan_to_num(im, copy=False)\n\n            autocorr = np.abs(\n                cross_correlate_masked(arr1=im, arr2=im, m1=mask, m2=mask, mode=\"same\")\n            )\n            autocorr = shift(\n                autocorr,\n                shift=np.asarray(center) - np.array(im.shape) / 2,\n                order=1,\n                mode=\"nearest\",\n            )\n            laplacian = -1 * laplace(autocorr)\n            threshold = filters.threshold_triangle(laplacian)\n            regions = (laplacian > threshold) * mask\n\n            # To prevent noise from looking like actual peaks,\n            # we erode labels using a small selection area\n            regions = binary_erosion(regions, footprint=disk(2))\n\n            labels = label(regions, return_num=False)\n            props = regionprops(label_image=labels, intensity_image=im)\n            candidates = [\n                prop for prop in props if not np.any(np.isnan(prop.weighted_centroid))\n            ]\n            peaks = list()\n            for prop in candidates:\n                pos = np.asarray(prop.weighted_centroid)\n                if any((np.linalg.norm(peak - pos) < min_dist) for peak in peaks):\n                    continue\n                else:\n                    if prop.area_convex < 0.2 * np.prod(image.shape):\n                        for coord in prop.coords:\n                            blob_mask[coord[0], coord[1]] = 1\n                    peaks.append(pos[::-1])\n            peaks = np.array([peaks]).reshape(\n                -1, 2\n            )  # now gives us the final array of peaks\n        case \"persistence\":  # we use the persistence algorithm\n            g0 = Persistence(image).persistence\n            birth_death = list()\n            birth_death_indices = list()\n            persistencies = list()\n            candidates = list()\n            bd_threshold = 0.0\n            for i, homclass in enumerate(g0):\n                p_birth, bl, pers, p_death = homclass\n                persistencies.append(pers)\n                if pers <= bd_threshold:\n                    continue\n                x, y = bl, bl - pers\n                birth_death.append([x, y])\n                birth_death_indices.append(i)\n            for i, homclass in enumerate(g0):\n                p_birth, bl, pers, p_death = homclass\n                if pers <= prominence:\n                    continue\n                y, x = p_birth\n                candidates.append([x, y])\n            if min_dist > 0.0:\n                combos = combinations(candidates, 2)\n                points_to_remove = [\n                    point2\n                    for point1, point2 in combos\n                    if np.linalg.norm(np.array(point1) - np.array(point2)) < min_dist\n                ]\n                candidates = [\n                    point for point in candidates if point not in points_to_remove\n                ]\n            peaks = np.array(candidates).reshape(-1, 2)\n            # remove peaks that are within the masked area\n            if mask.sum() != mask.shape[0] * mask.shape[1]:\n                peaks = np.array([p for p in candidates if mask[p[1], p[0]]]).reshape(\n                    -1, 2\n                )\n\n        case \"log\":\n            # This is the most expensive algorithm!!\n            # Use only when dire!\n\n            # Step 1: we need to define the lapacian of a gaussian for a given sigma\n            def laplacian_of_gaussian(\n                sigma: float,\n            ):  # first define the laplacian of a gaussian filter\n                # window size\n                n = np.ceil(sigma * 6)\n                y, x = np.ogrid[-n // 2 : n // 2 + 1, -n // 2 : n // 2 + 1]\n                y_filter = np.exp(-(y**2 / (2.0 * sigma * sigma)))\n                x_filter = np.exp(-(x**2 / (2.0 * sigma * sigma)))\n                return (\n                    (-(2 * sigma**2) + (x * x + y * y))\n                    * (x_filter * y_filter)\n                    * (1 / (2 * np.pi * sigma**4))\n                )\n\n            # Step 2: perform the convolution with the LOG filters for increasing powers of sigma\n            log_images = np.zeros((max_power, *image.shape))  # to store responses\n            with scipy.fft.set_backend(customFFTBackend):\n                for i in range(max_power):\n                    log_images[i, ...] = np.square(\n                        fftconvolve(\n                            image,\n                            laplacian_of_gaussian(sigma * np.power(k, i)),\n                            mode=\"same\",\n                        )\n                    )  # squaring the response\n            # Step 3: detect the blobs\n            peaks_with_radius = list()\n            (h, w) = image.shape\n            for i in range(1, h):\n                for j in range(1, w):\n                    slice_img = log_images[:, i - 1 : i + 2, j - 1 : j + 2]\n                    result = np.amax(slice_img)\n                    # result_1 = np.amin(slice_img)\n                    if result >= prominence:\n                        z, x, y = np.unravel_index(slice_img.argmax(), slice_img.shape)\n                        peaks_with_radius.append((i + x - 1, j + y - 1, k**z * sigma))\n            candidates = np.array(list(set(peaks_with_radius)))\n            if min_dist > 0.0:\n                sigma = candidates[:, -1].max()\n                distance = 2 * sigma * np.sqrt(candidates.shape[1] - 1)\n                tree = spatial.cKDTree(candidates[:, :-1])\n                pairs = np.array(list(tree.query_pairs(distance)))\n                if len(pairs) == 0:\n                    return candidates\n                else:\n                    for i, j in pairs:\n                        blob1, blob2 = candidates[i], candidates[j]\n                        if np.linalg.norm(blob1[:-1] - np.array(blob2[:-1])) < min_dist:\n                            if blob1[-1] > blob2[-1]:\n                                blob2[-1] = 0\n                            else:\n                                blob1[-1] = 0\n                candidates = np.array([b for b in candidates if b[-1] > 0])\n            # Step 4: This method needs to be robust under the situation of contrast issues\n            # To get around this, we now pass these blobs to the standard blob detection\n            # algorithms to get the centroid of each contour now that we've identified\n            # regions of interest\n            for blob in candidates:\n                y, x, r = blob\n                rr, cc = ellipse(x, y, r / 2, r / 2, shape=blob_mask.shape)\n                blob_mask[rr, cc] = 1\n            labels = label(blob_mask)\n            rprops = regionprops(label_image=labels, intensity_image=blob_mask)\n            peaks = list()\n            for region in [\n                prop for prop in rprops if not np.any(np.isnan(prop.weighted_centroid))\n            ]:\n                peaks.append(region.weighted_centroid)\n            peaks = np.array(peaks).reshape(-1, 2)\n    # Congratulations! We now have an (N,2) array of the peaks in the image;\n    # let's visualize it!\n    # Right now we have a greyscale image, let's create a version\n    # that's black and white so we can render it. First, scale image to\n    # range 0-255.\n\n    rgb_image = np.zeros(\n        (*image.shape, 3), dtype=np.uint8\n    )  # only generated for plotting\n    rgb_image[..., 0] = image * 255  # Red channel\n    rgb_image[..., 1] = image * 255  # Green channel\n    rgb_image[..., 2] = image * 255  # Blue channel\n\n    layout = plot_layout(title=f\"IMAGE with {peaks.shape[0]} objects\")\n    fig = px.imshow(img=rgb_image)\n    fig.layout = layout\n    marker_trace = go.Scatter(\n        x=peaks[:, 0],\n        y=peaks[:, 1],\n        mode=\"markers\",\n        marker=dict(color=\"green\", size=15),\n        showlegend=False,\n    )\n    fig.add_trace(marker_trace)\n\n    fig.update_xaxes(range=[image.shape[0], 0])\n    fig.update_yaxes(range=[image.shape[1], 0])\n    return EXTREMA_OUTPUT(fig=Plotly(fig=fig), blobs=Grayscale(img=blob_mask))\n\n\nclass UnionFind:\n\n    \"\"\"Union-find data structure.\n\n    Each unionFind instance X maintains a family of disjoint sets of\n    hashable objects, supporting the following two methods:\n\n    - X[item] returns a name for the set containing the given item.\n      Each set is named by an arbitrarily-chosen one of its members; as\n      long as the set remains unchanged it will keep the same name. If\n      the item is not yet part of a set in X, a new singleton set is\n      created for it.\n\n    - X.union(item1, item2, ...) merges the sets containing each item\n      into a single larger set.  If any item is not yet part of a set\n      in X, it is added to X as one of the members of the merged set.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Create a new empty union-find structure.\"\"\"\n        self.weights = {}\n        self.parents = {}\n\n    def add(self, object, weight):\n        if object not in self.parents:\n            self.parents[object] = object\n            self.weights[object] = weight\n\n    def __contains__(self, object):\n        return object in self.parents\n\n    def __getitem__(self, object):\n        \"\"\"Find and return the name of the set containing the object.\"\"\"\n\n        # check for previously unknown object\n        if object not in self.parents:\n            assert False\n            self.parents[object] = object\n            self.weights[object] = 1\n            return object\n\n        # find path of objects leading to the root\n        path = [object]\n        root = self.parents[object]\n        while root != path[-1]:\n            path.append(root)\n            root = self.parents[root]\n\n        # compress the path and return\n        for ancestor in path:\n            self.parents[ancestor] = root\n        return root\n\n    def __iter__(self):\n        \"\"\"Iterate through all items ever found or unioned by this structure.\"\"\"\n        return iter(self.parents)\n\n    def union(self, *objects):\n        \"\"\"Find the sets containing the objects and merge them all.\"\"\"\n        roots = [self[x] for x in objects]\n        heaviest = max([(self.weights[r], r) for r in roots])[1]\n        for r in roots:\n            if r != heaviest:\n                self.parents[r] = heaviest\n\n\nclass Persistence:\n    def __init__(self, im):\n        self.image = im\n        self.calculate()\n\n    def calculate(self):\n        h, w = self.image.shape\n\n        # Get indices orderd by value from high to low\n        indices = [(i, j) for i in range(h) for j in range(w)]\n        indices.sort(key=lambda p: self.get(p), reverse=True)\n\n        # Maintains the growing sets\n        self.uf = UnionFind()\n\n        self._groups0 = {}\n\n        # Process pixels from high to low\n        for i, p in enumerate(indices):\n            v = self.get(p)\n            ni = [self.uf[q] for q in self.iter_neighbors(p, w, h) if q in self.uf]\n            nc = sorted([(self.get_comp_birth(q), q) for q in set(ni)], reverse=True)\n\n            if i == 0:\n                self._groups0[p] = (v, v, None)\n\n            self.uf.add(p, -i)\n\n            if len(nc) > 0:\n                oldp = nc[0][1]\n                self.uf.union(oldp, p)\n\n                # Merge all others with oldp\n                for bl, q in nc[1:]:\n                    if self.uf[q] not in self._groups0:\n                        # print(i, \": Merge\", uf[q], \"with\", oldp, \"via\", p)\n                        self._groups0[self.uf[q]] = (bl, bl - v, p)\n                    self.uf.union(oldp, q)\n\n        self._groups0 = [\n            (k, self._groups0[k][0], self._groups0[k][1], self._groups0[k][2])\n            for k in self._groups0\n        ]\n        self._groups0.sort(key=lambda g: g[2], reverse=True)\n        self.persistence = self._groups0\n\n    def get_comp_birth(self, p):\n        return self.get(self.uf[p])\n\n    def get(self, p):\n        return self.image[p[0]][p[1]]\n\n    def iter_neighbors(self, p, w, h):\n        y, x = p\n\n        # 8-neighborship\n        neigh = [(y + j, x + i) for i in [-1, 0, 1] for j in [-1, 0, 1]]\n        # 4-neighborship\n        # neigh = [(y-1, x), (y+1, x), (y, x-1), (y, x+1)]\n\n        for j, i in neigh:\n            if j < 0 or j >= h:\n                continue\n            if i < 0 or i >= w:\n                continue\n            if j == y and i == x:\n                continue\n            yield j, i\n\n\nclass customFFTBackend:\n    \"\"\"\n    Pocket fft is usually a lot faster for this type of registration.\n    This class simply provides the implementation of the uarray protocol\n    to the scipy context manager.\n    \"\"\"\n\n    __ua_domain__ = \"numpy.scipy.fft\"\n\n    @staticmethod\n    def __ua_function__(method, args, kwargs):\n        fn = getattr(_pocketfft, method.__name__, None)\n\n        if fn is None:\n            return NotImplemented\n        workers = kwargs.pop(\"workers\", cpu_count())\n        return fn(*args, workers=workers, **kwargs)\n\n\ndef autocenter(im: DCNpArrayType, mask: Optional[DCNpArrayType] = None):\n    im = np.array(im, copy=True, dtype=float)\n    im -= im.min()\n    if mask is None:\n        mask = np.ones_like(im, dtype=bool)\n        weights = im\n    else:\n        weights = im * mask.astype(im.dtype)\n    rr, cc = np.indices(im.shape)\n    r_ = int(np.average(rr, weights=weights))\n    c_ = int(np.average(cc, weights=weights))\n    # Determine the smallest center -> side distance, and crop around that\n    # 1. Some images are not centered, and so there's a lot\n    # of image area that cannot be used for registration.\n    # 2. radial inversion becomes simple inversion of dimensions\n    side_length = floor(min([r_, abs(r_ - im.shape[0]), c_, abs(c_ - im.shape[1])]))\n    rs = slice(r_ - side_length, r_ + side_length)\n    cs = slice(c_ - side_length, c_ + side_length)\n    im = im[rs, cs]\n    mask = mask[rs, cs]\n\n    # Certain images display a gradient in the overall intensity\n    # For this purpose, we normalize the intensity by some \"background\",\n    # i.e. very blurred diffraction pattern\n    with catch_warnings():\n        simplefilter(\"ignore\", category=RuntimeWarning)\n        im /= gaussian_filter(input=im, sigma=min(im.shape) / 25, truncate=2)\n    im = np.nan_to_num(im, copy=False)\n\n    # The comparison between Friedel pairs from [1] is generalized to\n    # any inversion symmetry.\n    im_i = im[::-1, ::-1]\n    mask_i = mask[::-1, ::-1]\n\n    # masked normalized cross-correlation is extremely expensive\n    # we therefore downsample large images for essentially identical result\n    # but ~4x decrease in processing time\n    downsampling = 1\n    if min(im.shape) > 1024:\n        downsampling = 2\n\n    with scipy.fft.set_backend(customFFTBackend):\n        shift, *_ = phase_cross_correlation(\n            reference_image=im[::downsampling, ::downsampling],\n            moving_image=im_i[::downsampling, ::downsampling],\n            reference_mask=mask[::downsampling, ::downsampling],\n            moving_mask=mask_i[::downsampling, ::downsampling],\n            return_error=\"always\",\n        )\n    # Because images were downsampled, the correction\n    # factor to the rough center should be increased from the measured shift\n    correction = shift * downsampling\n\n    return np.array([r_, c_]) + correction / 2\n"
}